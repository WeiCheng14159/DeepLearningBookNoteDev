{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chap 6 Deep Feedforward Networks\n",
    "\n",
    "Deep feedforward networks, also often called __feedforward neural networks__, or __multilayer perceptrons__ (MLPs), are the quintessential deep learning models. The goal of a feedforward network is to approximate some function $f^{*}$.\n",
    "\n",
    "These models are called __feedforward__ because information flows through the function being evaluated from x, through the intermediate computations used to define f, and finally to the output y. \n",
    "\n",
    "Feedforward neural networks are called __networks__ because they are typically represented by composing together many different functions.\n",
    "\n",
    "One way to understand feedforward networks is to begin with linear models (logistic regression, linear regression) and consider how to overcome their limitations. \n",
    "\n",
    "- Pros: Efficient (Solved by closed form or convex optimization)\n",
    "- Cons: Model capacity limited to linear function \n",
    "\n",
    "To extend linear models to represent nonlinear functions of x, we can apply the linear model not to x itself but to a transformed input φ(x), where φ is a nonlinear transformation.\n",
    "\n",
    "Options for the mapping φ.\n",
    "- Very generic φ\n",
    "   - If φ(x) is of high enough dimension, we can always have enough capacity to fit the training set, but __generalization__ to the test set often remains __poor__.\n",
    "   - Very generic feature mappings are usually based only on the principle of local smoothness and __do not encode enough prior information__ to solve advanced problems.\n",
    "\n",
    "- Manually engineer φ\n",
    "    - This approach requires decades of __human effort__ for each separate task, with practitioners specializing in different domains such as speech recognition or computer vision, and with __little transfer between domains__.\n",
    "    \n",
    "- Deep learning's approach to learn φ\n",
    "    - This approach is the only one of the three that __gives up on the convexity__ of the training problem, but the benefits outweigh the harms.\n",
    "    - This approach can capture the benefit of the first approach by being __highly generic__ by using a very broad family φ(x;θ)\n",
    "    - Human practitioners can encode their knowledge to help generalization by __designing families φ(x; θ)__ that they expect will perform well.\n",
    "    - The advantage is that the human designer only needs to find the right general function family rather than finding precisely the right function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Example: Learning XOR\n",
    "The XOR function provides the target function $y = f^{∗} (x)$ that we want to learn. Our model provides a function $y = f(x;\\theta)$ and our learning algorithm will adapt the parameters $\\theta$ to make $f$ as similar as possible to $f^{*}$\n",
    "\n",
    "__Training Data__ \n",
    "$$\n",
    "\\mathbb{X} = \n",
    "    \\begin{bmatrix}\n",
    "        0 & 0 & 1 & 1\\\\\n",
    "        0 & 1 & 0 & 1\\\\\n",
    "    \\end{bmatrix}, \n",
    "\\mathbb{Y} =\n",
    "    \\begin{bmatrix}\n",
    "        0 & 1 & 1 & 0\\\\\n",
    "    \\end{bmatrix} \n",
    "$$\n",
    "\n",
    "__Loss Function__ \n",
    "$$\\mathbb{J}(\\theta) = \\frac{1}{4} \\sum_{x \\in \\mathbb{X}} (f^{*}(x)-f(x;\\theta))^2$$\n",
    "\n",
    "__Linear Model__\n",
    "$$f(x;\\omega,b) = x^T\\omega+b$$\n",
    "\n",
    "__Model 1: Linear Regression__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# XOR logic \n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = np.array([0,1,1,0])\n",
    "\n",
    "# Train linear regression\n",
    "reg = LinearRegression().fit(X, Y)\n",
    "\n",
    "# Make prediction \n",
    "Y_pred = reg.predict(X)\n",
    "\n",
    "# Plot\n",
    "plt.scatter(X[1:3,:1], X[1:3,1:],  color='red')\n",
    "plt.scatter(X[::3,:1], X[::3,1:],  color='black')\n",
    "plt.plot(X, Y_pred, color='blue', linewidth=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Model 2: Nonlinear Function (ReLU)__\n",
    "\\begin{align}\n",
    "    h &= f^{(1)}(x;W,c), y = f^{(2)}(h;w,b) \\\\\n",
    "    h &= g(W^Tx+c), y = w^Th+b \\\\\n",
    "    g &(z) = max\\{0,z\\} \\\\ \n",
    "    f &(x;W,c,w,b) = w^T max\\{0, W^Tx+c\\}+b \\\\\n",
    "\\end{align}\n",
    "\n",
    "Training Data \n",
    "$$\n",
    "\\mathbb{X} = \n",
    "    \\begin{bmatrix}\n",
    "        0 & 0 & 1 & 1 \\\\\n",
    "        0 & 1 & 0 & 1 \\\\\n",
    "    \\end{bmatrix}, \n",
    "\\mathbb{Y} =\n",
    "    \\begin{bmatrix}\n",
    "        0 & 1 & 1 & 0 \\\\\n",
    "    \\end{bmatrix} \n",
    "$$\n",
    "Train\n",
    "$$\n",
    "W = \\begin{bmatrix} \n",
    "        1 & 1 \\\\\n",
    "        1 & 1 \\\\\n",
    "    \\end{bmatrix} \n",
    "c = \\begin{bmatrix} \n",
    "        0  \\\\\n",
    "        -1 \\\\\n",
    "    \\end{bmatrix} \n",
    "w = \\begin{bmatrix} \n",
    "        1  \\\\\n",
    "        -2 \\\\\n",
    "    \\end{bmatrix} \n",
    "b = \\begin{bmatrix} \n",
    "    0 & 0 & 0 & 0 \\\\\n",
    "\\end{bmatrix} \n",
    "    \\\\\n",
    "WX = \n",
    "    \\begin{bmatrix} \n",
    "        0 & 1 & 1 & 2 \\\\\n",
    "        0 & 1 & 1 & 2 \\\\\n",
    "    \\end{bmatrix} \\\\\n",
    "WX + c = \n",
    "    \\begin{bmatrix} \n",
    "        0  & 1 & 1 & 2 \\\\\n",
    "        -1 & 0 & 0 & 1 \\\\\n",
    "    \\end{bmatrix} \\\\\n",
    "h = \n",
    "    \\begin{bmatrix} \n",
    "        0  & 1 & 1 & 2 \\\\\n",
    "        0  & 0 & 0 & 1 \\\\\n",
    "    \\end{bmatrix} \\\\\n",
    "w^Th + b= \n",
    "\\begin{bmatrix} \n",
    "    0  & 1 & 1 & 0 \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "f (x;W,c,w,b) = \n",
    "\\begin{bmatrix} \n",
    "    0  & 1 & 1 & 0 \\\\\n",
    "\\end{bmatrix} = \\mathbb{Y}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Gradient-Based Learning\n",
    "The largest difference between the __linear models__ we have seen so far and __neural networks__ is that the __nonlinearity__ of a neural network causes most interesting loss functions to become __non-convex.__\n",
    "\n",
    "Neural networks are usually trained by using iterative, gradient-based optimizers that merely drive the cost function to a very low value \n",
    "\n",
    "Stochastic gradient descent applied to non-convex loss functions has no such convergence guarantee, and is __sensitive to the values of the initial parameters__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.1 Cost Functions\n",
    "Our parametric model defines a distribution $p(y|x;\\theta)$ and we simply use the principle of maximum likelihood. This means we use the __cross-entropy__ between the training data and the model’s predictions as the cost function.\n",
    "\n",
    "The total cost function used to train a neural network will often combine one of the primary cost functions described here with a __regularization term.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2.1.1 Learning Conditional Distributions with Maximum Likelihood\n",
    "\n",
    "Most modern neural networks are trained using __maximum likelihood__. This means that the cost function is simply the __negative log-likelihood__, equivalently described as the __cross-entropy__ between the training data and the model distribution.\n",
    "\n",
    "$$J(\\theta) = - \\mathbb{E}_{x,y \\sim \\hat p_{data}} \\log p_{model}(y|x)$$\n",
    "\n",
    "The specific form of the cost function changes from model to model, depending on the specific form of $\\log p_{model}$\n",
    "\n",
    "An advantage of this approach of deriving the cost function from __maximum likelihood__ is that it removes the burden of designing cost functions for each model. Specifying a model p(y | x) automatically determines a cost function log p(y | x).\n",
    "\n",
    "One recurring theme throughout neural network design is that the __gradient of the cost function__ must be __large and predictable__ enough to serve as a good guide for the learning algorithm. \n",
    "\n",
    "Functions that saturate (become very flat) undermine this objective because they make the gradient become very small. This happens because the activation functions used to produce the output of the hidden units or the output units __saturate__.\n",
    "\n",
    "The negative log-likelihood helps to avoid this problem for many models. Many output units involve an exp function that can saturate when its argument is very negative. The log function in the negative log-likelihood cost function undoes the exp of some output units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.1.2 Learning Conditional Statistics\n",
    "skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 Output Units\n",
    "The choice of __cost function__ is __tightly coupled__ with the choice of __output unit__. We simply use the __cross-entropy__ between the data distribution and the model distribution. The choice of how to represent the output then determines the form of the cross-entropy function.\n",
    "\n",
    "Any kind of neural network unit that may be used as an output can also be used as a hidden unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.2.1 Linear Units for Gaussian Output Distributions\n",
    "One simple kind of output unit is an output unit based on an affine transformation with __no nonlinearity__. These are often just called linear units. \n",
    "\n",
    "Because linear units do not saturate, they pose little difficulty for gradient-based optimization algorithms and may be used with a wide variety of optimization algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.2.2 Sigmoid Units for Bernoulli Output Distributions\n",
    "Many tasks require predicting the value of a __binary variable y__. __Classification problems__ with two classes can be cast in this form.\n",
    "\n",
    "A Bernoulli distribution is defined by just a single number. The neural net needs to predict only P(y = 1 | x).\n",
    "\n",
    "A sigmoid output unit is defined by $$\\hat y = \\sigma(w^Th+b)$$\n",
    "\n",
    "First, it uses a linear layer to compute $z = w^Th + b$. Next, it uses the __sigmoid__ activation function to __convert z into a probability.__ \n",
    "\n",
    "Let P(y) be normalized probability distribution, and y is a binary variable y=0 or y=1\n",
    "\n",
    "Let P(y=1) then $$P(y) = \\frac{e^z}{1+e^z} = \\sigma(z)$$\n",
    "\n",
    "Let P(y=0) then $$P(y) = \\frac{1}{1+e^z} = \\sigma(-z)$$\n",
    "\n",
    "Combined $$P(y) = \\sigma((2y-1)z)$$\n",
    "\n",
    "The detailed derivation can be found \n",
    "- https://towardsdatascience.com/understanding-the-motivation-of-sigmoid-output-units-e2c560d4b2c4\n",
    "- https://stats.stackexchange.com/questions/269575/motivating-sigmoid-output-units-in-neural-networks-starting-with-unnormalized-lo/361267#361267 \n",
    "\n",
    "Because the __cost function__ used with __maximum likelihood__ is −log P(y|x), the log in the cost function undoes the exp of the sigmoid. Without this effect, the saturation of the sigmoid could prevent gradient-based learning from making good progress.\n",
    "\n",
    "\\begin{align}\n",
    "    J(\\theta) &= - \\log P(y|x) \\\\\n",
    "              &= - \\log \\sigma((2y-1)z) \\\\\n",
    "              &= \\zeta((1-2y)z) \\\\\n",
    "\\end{align}\n",
    "\n",
    "By rewriting the loss in terms of the softplus function, we can see that it __saturates__ only when (1 − 2y)z is very __negative. __\n",
    "\n",
    "Saturation thus occurs only when the model already has the right answer when y = 1 and z is very positive, or y = 0 and z is very negative. \n",
    "\n",
    "When z has the wrong sign, the argument to the softplus function, (1−2y)z, may be simplified to |z|. As |z| becomes large while z has the wrong sign, the softplus function asymptotes toward simply returning its argument |z|.\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\zeta((1-2y)z) \\approx |z| \\\\\n",
    "\\frac{\\partial J(\\theta)}{\\partial z} \\approx sign(z) \\\\\n",
    "$$\n",
    "\n",
    "so, in case of extremely incorrect z, the softplus function __does not shrink the gradient at all.__ This property is very useful because it means that gradient-based learning can act to quickly correct a mistaken z.\n",
    "\n",
    "When we use other loss functions, such as mean squared error, the loss can saturate anytime σ(z) saturates. For this reason, maximum likelihood is almost always the preferred approach to training sigmoid\n",
    "output units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2.2.3 Softmax Units for Multinoulli Output Distributions\n",
    "\n",
    "Any time we wish to represent a probability distribution over a discrete variable with n possible values, we may use the __softmax function__.\n",
    "\n",
    "In the case of binary variables, we wished to produce a single number \n",
    "\n",
    "$$\\hat y = P (y = 1 | x)$$ \n",
    "\n",
    "Given input x, $\\hat y$ is an estimation of random variable y equals to 1. \n",
    "\n",
    "Because this number needed to lie between 0 and 1, and because we wanted the logarithm of the number to be well-behaved for gradient-based optimization of the log-likelihood. (In case the likelihood underflows to zero then the negative likelihood yields __negative infinity__)\n",
    "\n",
    "We chose to instead predict a number (in case y is a binary random variable) $$z = \\log \\tilde P(y = 1 | x)$$\n",
    "\n",
    "To generalize to the case of a discrete variable with n values, we now need to produce a vector $\\hat y$, with $$\\hat y_i = P(y = i|x)$$\n",
    "\n",
    "We require not only that each element of $\\hat y$ be between 0 and 1, but also that the entire vector sums to 1 so that i\n",
    "it represents a valid probability distribution.\n",
    "\n",
    "First, a linear layer predicts __unnormalized log probabilities__: $$z = W^Th + b$$ where $$z_i = \\log \\tilde P(y=i|x)$$\n",
    "\n",
    "\n",
    "> 將神經網路的線性輸出z_i定義成對 P(y=i|x) 取完 log 之值\n",
    "\n",
    "The softmax function can then exponentiate and normalize z to obtain the desired $\\hat y$ \n",
    "$$\\hat y = \\mathbb{softmax}(z)_i = \\frac{exp(z_i)}{\\sum_j exp(z_j)}$$\n",
    "\n",
    "> 因為z向量的每一個元素都是介於0-1的機率值（但元素和不為1），但是我們需要的是每一個元素相加等於1的機率，故我們使用softmax函數來標準化z，後得到向量y hat\n",
    "\n",
    "The use of the exp function works very well when training the softmax to output a target value y using maximum log-likelihood. We wish to maximize $$ \\log \\hat y_i = \\log P(y = i;z) = \\log  \\mathbb{softmax}(z)_i$$\n",
    "\n",
    "Defining the softmax in terms of exp is natural because the log in the log-likelihood can undo the exp of the softmax:\n",
    "$$\\log \\mathbb{softmax}(z)_i = z_i - \\log \\sum_j exp(z_j)$$\n",
    "\n",
    "When maximizing the log-likelihood, the first term encourages $z_i$ to be __pushed up__, while the second term encourages all of z to be __pushed down__.\n",
    "\n",
    "The first term shows that the input $z_i$ always has a direct contribution to the cost function. Because this term cannot saturate, we know that learning can proceed, even if the contribution of $z_i$ to the second term becomes very small.\n",
    "\n",
    "Observed that the second term $\\log \\sum_j exp(z_j)$ can be roughly approximately by $\\max_j z_j$ This approximation is based on the idea that $exp(z_k)$ is insignificant for any $z_k$ that is noticeably less than $\\max_j z_j$\n",
    "\n",
    "e.g. $\\log \\sum (1 + 10 + 100 + 10000) = \\log (10111) \\approx \\log 10^4 = 4$\n",
    "\n",
    "The intuition we can gain from this approximation is that the __negative log-likelihood cost function always strongly penalizes the most active incorrect prediction__\n",
    "\n",
    "\\begin{align}\n",
    "    -\\log \\mathbb{softmax}(z)_i &= - z_i + \\log \\sum_j exp(z_j) \\\\\n",
    "    & \\approx - z_i + max_j z_j \\\\ \n",
    "\\end{align}\n",
    "\n",
    "If the correct answer already has the largest input to the softmax, then the $−z_i$ term and the $\\log \\sum_{j} exp(z_j) \\approx max_j z_j = z_i$ terms will roughly cancel.\n",
    "\n",
    "Many objective functions other than the log-likelihood do not work as well with the softmax function. Specifically, objective functions that do NOT use a log to undo the exp of the softmax fail to learn when the argument to the exp becomes\n",
    "very negative, causing the gradient to vanish.\n",
    "\n",
    "In particular, squared error is a poor loss function for softmax units, and can fail to train the model to change its output, even when the model makes highly confident incorrect predictions. \n",
    "\n",
    "Like the sigmoid, the softmax activation can saturate. In the case of the softmax, there are multiple output values. These output values can saturate when the differences between input values become extreme. When the softmax saturates, many cost functions based on the softmax also saturate, unless they are able to invert the saturating activating function.\n",
    "\n",
    "Observe that the softmax output is invariant to adding the same scalar to all of its inputs: $$\\mathbb{softmax}(z) = \\mathbb{softmax}(z+c)$$\n",
    "\n",
    "A numerically stable variant of the softmax: $$\\mathbb{softmax}(z) = \\mathbb{softmax}(z - \\max_i z_i)$$\n",
    "\n",
    "The reformulated version allows us to evaluate softmax with only small numerical errors even when z contains extremely large or extremely negative numbers. We see that the softmax function is driven by the amount that its arguments deviate from $\\max_i z_i$\n",
    "\n",
    "Output $\\mathbb{softmax}(z)_i$ __saturates to 1__ when the corresponding input is maximal ($z_i = max_i z_i$ ) and $z_i$ is much greater than all of the other inputs. The output $\\mathbb{softmax}(z)_i$ can also __saturate to 0__ when $z_i$ is not maximal and the maximum is much greater. \n",
    "\n",
    "e.g. $z_1 = 1,z_2=10,z_3=100,z_4=10000$ \n",
    "$$\\mathbb{softmax}(z_1) = \\frac{z_1}{\\sum_j z_j} = \\frac{1}{10111} \\approx 0$$\n",
    "$$\\mathbb{softmax}(z_4) = \\frac{z_4}{\\sum_j z_j} = \\frac{10000}{10111} \\approx 1$$\n",
    "\n",
    "From a neuroscientific point of view, it is interesting to think of the softmax as a way to create a form of competition between the units that participate in it: the softmax outputs always sum to 1 so an increase in the value of one unit necessarily corresponds to a decrease in the value of others. \n",
    "\n",
    "The name “softmax” can be somewhat confusing. The function is more closely related to the argmax function than the max function. The term “soft” derives from the fact that the softmax function is continuous and differentiable. The arg max function, with its result represented as a one-hot vector, is not continuous or differentiable. The softmax function thus provides a “softened” version of the argmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2.2.4 Other Output Types\n",
    "Neural networks can generalize to almost any kind of output layer that we wish.\n",
    "\n",
    "If we define a conditional distribution p(y | x; θ), the principle of maximum likelihood suggests we use $− \\log p(y|x;\\theta)$ as our cost function.\n",
    "\n",
    "... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Hidden Units\n",
    "How to choose the type of hidden unit to use in the hidden layers of the model ? The design of hidden units is an extremely active area of research and does not yet have many definitive guiding theoretical principles.\n",
    "\n",
    "Rectified linear units are an excellent default choice of hidden unit. .. The design process consists of trial and error, intuiting that a kind of hidden unit may work well, and then training a network with that kind of hidden unit and evaluating its performance on a validation set.\n",
    "\n",
    "Some of the hidden units included in this list are not actually differentiable at all input points. This may seem like it invalidates g for use with a gradient-based learning algorithm. In practice, gradient descent still performs well enough for these models to be used for machine learning tasks. This is in part because neural network training algorithms do not usually arrive at a local minimum of the cost function, but instead merely reduce its value significantl. \n",
    "\n",
    "Hidden units that are not differentiable are usually non-differentiable at only a small number of points.\n",
    "\n",
    "Software implementations of neural network training usually return one of the one-sided derivatives rather than reporting that the derivative is undefined or raising an error.\n",
    "\n",
    "Unless indicated otherwise, most hidden units can be described as accepting a vector of inputs x, computing an affine transformation $z = W^Tx + b$, and then applying an element-wise nonlinear function g(z).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1 Rectified Linear Units and Their Generalizations\n",
    "$$g(z) = \\max \\{0, z\\}$$\n",
    "\n",
    "Rectified linear units are easy to optimize because they are so similar to linear units.\n",
    "\n",
    "The __gradients__ are not only __large__ but also __consistent.__ The __second derivative__ of the rectifying operation is __0__ almost everywhere, and the derivative of the rectifying operation is 1 everywhere that the unit is active.\n",
    "\n",
    "When initializing the parameters of the affine transformation, it can be a good practice to __set all elements of b to a small, positive value, such as 0.1.__ This makes it very likely that the rectified linear units will be __initially active__ for most inputs in the training set and allow the derivatives to pass through.\n",
    "\n",
    "One drawback to rectified linear units is that they cannot learn via gradient-based methods on examples for which their activation is zero.\n",
    "\n",
    "Rectified linear units and all of these generalizations of them are based on the principle that models are easier to optimize if their behavior is closer to linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2 Logistic Sigmoid and Hyperbolic Tangent\n",
    "Logistic sigmoid activation function $$g(z) = \\sigma(z)$$\n",
    "\n",
    "Hyperbolic tangent activation function $$g(z) = tanh(z)$$\n",
    "\n",
    "These activation functions are closely related because $tanh(z) = 2 \\sigma (2z) − 1$\n",
    "\n",
    "They saturate to a high value when z is very positive, saturate to a low value when z is very negative, and are only strongly sensitive to their input when z is near 0.\n",
    "\n",
    "Hyperbolic tangent activation function resembles the identity function more closely, in the sense that tanh(0) = 0 while $ \\sigma (0) = \\frac{1}{2}$. Because __tanh is similar to the identity function near 0__, training a deep neural network $\\hat y = w^T tanh(U ^T tanh(V^Tx))$ resembles training a linear model $\\hat y = w^TU^TV^Tx$ so long as the activations of the network can be kept small. This makes training the tanh network easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.3 Other Hidden Units\n",
    "In general, a wide variety of differentiable functions as hidden units perform perfectly well. Many unpublished activation functions perform just as well as the popular ones. \n",
    "\n",
    "To provide a concrete example, the authors tested a feedforward network using cos function as hidden unit on the MNIST dataset and obtained an error rate of less than 1%, which is competitive with results obtained using more conventional activation functions.\n",
    "\n",
    "One possibility is to not have an activation g (z) at all. If every layer of the neural network consists of only linear transformations, then the network as a whole will be linear. However, it is acceptable for some layers of the neural network to be purely linear.\n",
    "\n",
    "Linear hidden units thus offer an effective way of reducing the number of parameters in a network.\n",
    "\n",
    "Softmax units are another kind of unit that is usually used as an output but may sometimes be used as a hidden unit. Softmax units naturally represent a probability distribution over a discrete variable with k possible values, so they may be used as a kind of switch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Architecture Design\n",
    "The word __architecture__ refers to the overall structure of the network: how many units it should have and how these units should be connected to each other.\n",
    "\n",
    "In these chain-based architectures, the main architectural considerations are to choose the __depth__ of the network and the __width__ of each layer. \n",
    "\n",
    "__Deeper networks__ often are able to use far __fewer units per layer__ and far __fewer parameters__ and often generalize to the test set, but are also often harder to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.1 Universal Approximation Properties and Depth\n",
    "Feedforward networks with hidden layers provide a universal approxi- mation framework.\n",
    "\n",
    "Specifically, the __universal approximation theorem__ states that a feedforward network with a linear output layer and at least one hidden layer with any “squashing” activation function (such as the logistic sigmoid activation function) can approximate any __Borel measurable function__ from one finite-dimensional space to another with any desired non-zero amount of error, provided that the network is given enough hidden units.\n",
    "\n",
    "The concept of __Borel measurability__ is beyond the scope of this book; for our purposes it suffices to say that any continuous function on a closed and bounded subset of $R^n$ is Borel measurable and therefore may be approximated by a neural network.\n",
    "\n",
    "The __universal approximation theorem__ means that regardless of what function we are trying to learn, we know that a large MLP will be able to __represent__ this function. However, we are not guaranteed that the training algorithm will be able to __learn__ that function. \n",
    "\n",
    "Learning can fail for two different reasons. First, the optimization algorithm used for training may not be able to find the value of the parameters that corresponds to the desired function. Second, the training algorithm might choose the wrong function due to overfitting.\n",
    "\n",
    "In summary, a feedforward network with a single layer is sufficient to represent any function, but the layer may be infeasibly large and may fail to learn and generalize correctly. In many circumstances, using deeper models can reduce the number of units required to represent the desired function and can reduce the amount of generalization error.\n",
    "\n",
    "Showed that functions representable with a deep rectifier net can require an exponential number of hidden units with a shallow (one hidden layer) network.\n",
    "\n",
    "Figure 6.5 illustrates how a network with __absolute value rectification__ creates mirror images of the function computed on top of some hidden unit, with respect to the input of that hidden unit. Each hidden unit specifies __where to fold the input space__ in order to create __mirror responses__ (on both sides of the absolute value nonlinearity). By composing these folding operations, we obtain an __exponentially large__ number of piecewise linear regions which can capture all kinds of regular (e.g., repeating) patterns.\n",
    "\n",
    "![Geometric explanation](ref/Fig6.5.png)\n",
    "\n",
    "An intuitive, geometric explanation of the __exponential advantage of deeper rectifier networks__ formally by Montufar et al. (2014).\n",
    "\n",
    "__(Left)__ An absolute value rectification unit has the same output for every pair of mirror points in its input. The mirror axis of symmetry is given by the hyperplane defined by the weights and bias of the unit. A function computed on top of that unit (the green decision surface) will be a mirror image of a simpler pattern across that axis of symmetry.\n",
    "\n",
    "__(Center)__ The function can be obtained by folding the space around the axis of symmetry. \n",
    "\n",
    "__(Right)__ Another repeating pattern can be folded on top of the first (by another downstream unit) to obtain another symmetry (which is now repeated four times, with two hidden layers). \n",
    "\n",
    "We may also want to choose a deep model for statistical reasons. Any time we choose a specific machine learning algorithm, we are __implicitly stating some set of prior beliefs__ we have about what kind of function the algorithm should learn. (e.g. Image recognition solved by CNN, Speech recognition solved by LSTM) Choosing a __deep model__ encodes a __very general belief__ that the function we want to learn should involve __composition of several simpler functions.__\n",
    "\n",
    "Empirically, greater depth does seem to result in better generalization for a wide variety of tasks. Using __deep architectures__ does indeed __express a useful prior__ over the space of functions the model learns.\n",
    "\n",
    "![Deeper Network Better Results](ref/Fig6.6.png)\n",
    "\n",
    "![Layers and Results](ref/Fig6.7.png)\n",
    "\n",
    "Deeper models tend to perform better. This is NOT merely because the model is larger. This experiment from Goodfellow et al. (2014d) shows that __increasing the number of parameters in layers of convolutional networks without increasing their depth is not nearly as effective at increasing test set performance.__ We observe that shallow models in this context overfit at around 20 million parameters while deep ones can benefit from having over 60 million. \n",
    "\n",
    "This suggests that using a deep model expresses a useful preference over the space of functions the model can learn. Specifically, it expresses a belief that __the function should consist of many simpler functions composed together.__ This could result either in learning a representation that is composed in turn of simpler representations or in learning a program with sequentially dependent steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.2 Other Architectural Considerations\n",
    "Another key consideration of architecture design is exactly how to connect a pair of layers to each other. \n",
    "\n",
    "Many specialized networks in the chapters ahead have __fewer connections__, so that each unit in the input layer is connected to only a small subset of units in the output layer. These strategies for reducing the number of connections reduce the number of parameters and the amount of computation required to evaluate the network, but are often __highly problem-dependent.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Back-Propagation and Other Differentiation Algorithms\n",
    "\n",
    "### 6.5.1 Computational Graphs\n",
    "Here, we use each __node__ in the graph to indicate a __variable.__ The variable may be a __scalar, vector, matrix, tensor__, or even a variable of another type. An __operation__ is a simple function of one or more variables. Functions more complicated than the operations in this set may be described by composing many operations together. Without loss of generality, we define an operation to return only a single output variable. This does not lose generality because the output variable can have multiple entries, such as a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.2 Chain Rule of Calculus\n",
    "Back-propagation is an algorithm that computes the chain rule, with a specific order of operations that is highly efficient.\n",
    "\n",
    "Suppose that $x \\in R^m, y \\in R^n$, g maps from $R^m$ to $R^n$, and f maps from $R^n$ to $R$. If y = g(x) and z = f(y), then $$\\frac{\\partial z}{\\partial x_i} = \\sum_j \\frac{\\partial z}{\\partial y_i} \\frac{\\partial y_i}{\\partial x_i}$$\n",
    "\n",
    "In vector notation, this may be equivalently written as $$\\nabla x^z = (\\frac{\\partial y}{\\partial x})^T \\nabla y^z$$\n",
    "where $\\frac{\\partial y}{\\partial x}$ is the n x m __Jacobian__ matrix of g\n",
    "\n",
    "The back-propagation algorithm consists of performing such a Jacobian-gradient product for each operation in the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.3 Recursively Applying the Chain Rule to Obtain Backprop \n",
    "Many subexpressions may be repeated several times within the overall expression for the gradient. Any procedure that computes the gradient will need to choose whether to store these subexpressions or to recompute them several times.\n",
    "\n",
    "For complicated graphs, there can be exponentially many of these wasted computations, making a naive implementation of the chain rule infeasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.4 Back-Propagation Computation in Fully-Connected MLP\n",
    "skip "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.5 Symbol-to-Symbol Derivatives\n",
    "__Algebraic expressions__ and __computational graphs__ both operate on __symbols__, or __variables__ that do not have specific values. These algebraic and graph-based representations are called __symbolic representations.__\n",
    "\n",
    "Some approaches to back-propagation take a __computational graph__ and a set of __numerical values for the inputs__ to the graph, then return a set of numerical values describing the gradient at those input values. We call this approach “symbol- to-number” differentiation. i.e. Torch, Caffe\n",
    "\n",
    "Another approach is to take a __computational graph__ and __add additional nodes__ to the graph that provide a symbolic description of the desired derivatives. The primary advantage of this approach is that the derivatives are described in the same language as the original expression. Because the derivatives are just another computational graph, it is possible to run back-propagation again, differentiating the derivatives in order to obtain __higher derivatives.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.7 Example: Back-Propagation for MLP Training\n",
    "As an example, we walk through the __back-propagation__ algorithm as it is used to train a __multilayer perceptron.__\n",
    "\n",
    "- __Model parameter:__ \n",
    "    - 1 hidden layer (activation function: ReLU)\n",
    "    - output layer (Minimizing Cross Entropy = Perform maximum likelihood estimation) \n",
    "\n",
    "- __Regularlization: L2 norm__ $$J = J_{mle} + \\lambda \\left ( \\sum_{i,j} \\left( W^{(1)}_{i,j} \\right)^2 + \\sum_{i,j} \\left(W^{(2)}_{i,j} \\right)^2 \\right ) $$\n",
    "\n",
    "- __Optimizer:__ SGD with minibatch. (Back prop is used to compute the gradient of a minibatch) \n",
    "\n",
    "To train, we wish to compute both $\\nabla_{W^{(1)}} J$ and $\\nabla_{W^{(2)}} J$. There are two different paths leading backward from J to the weights. Let $G$ be the gradient on the unnormalized log probabilities $U^{(2)}$ provided by the cross_entropy operation. $G = \\frac{\\partial J_{mle}}{\\partial U^{(2)}}$ Let $G'$ be the gradient on the ReLU operation. $G' = \\frac{\\partial H}{\\partial U^{(1)}}$\n",
    "- Through the cross-entropy cost ($J_{mle}$)\n",
    "\\begin{align}\n",
    "    \\nabla_{W^{(2)}} J_{mle} \n",
    "    =& \\frac{\\partial J_{mle}}{\\partial W^{(2)}} \\\\\n",
    "    =& \\frac{\\partial J_{mle}}{\\partial U^{(2)}} \\frac{\\partial U^{(2)}}{\\partial W^{(2)}} \\\\\n",
    "    =& G \\frac{\\partial U^{(2)}}{\\partial W^{(2)}} \\\\\n",
    "    =& H^TG \\\\\n",
    "    \\nabla_{W^{(1)}} J_{mle} \n",
    "    =& \\frac{\\partial J_{mle}}{\\partial W^{(1)}} \\\\ \n",
    "    =& \\frac{\\partial J_{mle}}{\\partial H} \\frac{\\partial H}{\\partial W^{(1)}} \\\\ \n",
    "    =& \\nabla_{H} J_{mle} \\frac{\\partial H}{\\partial W^{(1)}} \\\\ \n",
    "    =& G \\left(W^{(2)} \\right)^T \\frac{\\partial H}{\\partial W^{(1)}} \\\\ \n",
    "    =& G \\left(W^{(2)} \\right)^T \\frac{\\partial H}{\\partial U^{(1)}} \\frac{\\partial U^{(1)}}{\\partial W^{(1)}} \\\\\n",
    "    =& G \\left(W^{(2)} \\right)^T G' \\frac{\\partial U^{(1)}}{\\partial W^{(1)}} \\\\\n",
    "    =& G \\left(W^{(2)} \\right)^T X^T G' \\\\\n",
    "\\end{align}\n",
    "\n",
    "- Through the weight decay cost ($J_{Reg}$)\n",
    "    - $\\nabla_{W^{(1)}} J_{Reg} = 2 \\lambda \\sum_{i,j} W^{(1)}_{i,j} = 2 \\lambda W^{(1)}$\n",
    "    - $\\nabla_{W^{(2)}} J_{Reg} = 2 \\lambda \\sum_{i,j} W^{(2)}_{i,j} = 2 \\lambda W^{(2)}$\n",
    "- Combined \n",
    "    - $W^{(1)}_{new} = W^{(1)} + \\left(\\nabla_{W^{(1)}} J_{mle} + \\nabla_{W^{(1)}} J_{Reg} \\right) = W^{(1)} + \\left(G \\left(W^{(2)} \\right)^T X^T G' + 2 \\lambda W^{(1)}\\right)$\n",
    "    - $W^{(2)}_{new} = W^{(2)} + \\left(\\nabla_{W^{(2)}} J_{mle} + \\nabla_{W^{(2)}} J_{Reg} \\right)= W^{(2)} + \\left(H^TG + 2 \\lambda W^{(2)} \\right)$ \n",
    "\n",
    "![Computational Graph](ref/Fig6.11.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.8 Complications\n",
    "Our description of the back-propagation algorithm here is simpler than the implementations actually used in practice. \n",
    "- __Return type:__ As noted above, we have restricted the definition of an operation to be a function that returns a single tensor. Most software implementations need to support operations that can return more than one tensor.\n",
    "- __Memory bottleneck__: Back-propagation often involves summation of many tensors together. In the naive approach, each of these tensors would be computed separately, then all of them would be added in a second step. The naive approach has an overly high memory bottleneck that can be avoided by maintaining a single buffer and adding each value to that buffer as it is computed.\n",
    "- __Multiple datatype:__ Real-world implementations of back-propagation also need to handle various data types, such as 32-bit floating point, 64-bit floating point, and integer values.\n",
    "- __Boundary cases:__ Some operations have undefined gradients, and it is important to track these cases and determine whether the gradient requested by the user is undefined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.9 Differentiation outside the Deep Learning Community\n",
    "The back-propagation algorithm described here is only one approach to automatic differentiation. It is a special case of a broader class of techniques called __reverse mode accumulation__\n",
    "\n",
    "In general, __determining the order of evaluation__ that results in the __lowest computational cost__ is a __difficult__ problem. Finding the optimal sequence of operations to compute the gradient is __NP-complete__ (Naumann, 2008), in the sense that it may require simplifying algebraic expressions into their least expensive form.\n",
    "\n",
    "For example, suppose we have variables $p_1,p_2,...,p_n$ representing probabilities and variables $z_1, z_2 , . . . , z_n$ representing unnormalized log probabilities. \n",
    "\n",
    "Suppose we define a __softmax__ $$q_i = \\frac{exp(z_i)}{\\sum_i exp(z_i)} = \\frac{exp(z_i)}{Z}$$\n",
    "\n",
    "Construct a __cross-entropy loss__ \n",
    "\\begin{align} \n",
    "J(p|q)  &= - \\sum_i p_i \\log q_i \\\\ \n",
    "        &= - \\sum_i p_i (z_i - \\log Z) \\\\\n",
    "\\end{align}\n",
    "\n",
    "> Cross entropy: $H(P,Q) = - \\mathbb{E}_{x \\sim P} \\log Q(x) = \\sum_x - P(x) \\cdot\\log Q(x)$\n",
    "\n",
    "A human mathematician can observe that the derivative of J with respect to $z_i$ takes a very simple form: $\\frac{\\partial J}{\\partial z_i} =q_i − p_i$ \n",
    "\n",
    "\\begin{align} \n",
    "\\frac{\\partial J}{\\partial z_i} &= \\frac{\\partial}{\\partial z_i} \\left( \\sum_j p_j \\left( - z_j + \\log Z \\right) \\right)\\\\\n",
    "    &= \\sum_j \\left( p_j \\right) \\cdot \\frac{\\partial}{\\partial z_i} \\left(\\log Z\\right) - p_i \\\\\n",
    "    &= \\sum_j \\left( p_j \\right) \\cdot \\frac{1}{Z} \\frac{\\partial Z}{\\partial z_i} - p_i \\\\\n",
    "    & (Since \\; \\sum_j p_j = 1) \\\\\n",
    "    &= \\frac{1}{Z} e^{z_i} - p_i \\\\ \n",
    "    &= q_i - p_i \\\\\n",
    "\\end{align}\n",
    "\n",
    "The back-propagation algorithm is __NOT__ capable of simplifying the gradient this way, and will instead explicitly propagate gradients through all of the logarithm and exponentiation operations in the original graph. \n",
    "\n",
    "The number of gradient computed by back-prop is therefore O(E) where __E is the number of ALL possible edges from node i to node j__ However, it can potentially be reduced by simplifying the computational graph constructed by back-propagation, and this is an NP-complete task.\n",
    "\n",
    "When the number of outputs of the graph is larger than the number of inputs, it is sometimes preferable to use another form of automatic differentiation called __forward mode accumulation.__\n",
    "\n",
    "The relationship between forward mode and backward mode is analogous to the relationship between left-multiplying versus right-multiplying a sequence of matrices. \n",
    "\n",
    "Let A,B,C be 3x3 matrices, and D be 3x1 matrix. Assume the computational graph needs to compute forward-prop and back-prop. The number of computation of forward-prop and back-prop is as follow. \n",
    "\n",
    "|           | mat2mat $O(n^3)$ | mat2vec $O(n^2)$ |\n",
    "|-----------|------------------|------------------|\n",
    "| forw-prop | 2                | 1                |\n",
    "| back-prop | 0                | 3                |\n",
    "\n",
    "|           | Num of mul |\n",
    "|-----------|------------|\n",
    "| forw-prop | 63         |\n",
    "| back-prop | 27         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.10 Higher-Order Derivatives\n",
    "Instead of explicitly computing the Hessian, the typical deep learning approach is to use __Krylov methods.__ Krylov methods are a set of iterative techniques for performing various operations like approximately inverting a matrix or finding approximations to its eigenvectors or eigenvalues, without using any operation other than matrix-vector products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 Historical Notes\n",
    "The __chain rule__ that underlies the back-propagation algorithm was invented in the 17th century (Leibniz, 1676; L’Hôpital, 1696)\n",
    "\n",
    "__Gradient descent__ was not introduced as a technique for iteratively approximating the solution to optimization problems until the 19th century (Cauchy, 1847).\n",
    "\n",
    "__Perceptron__ were based on linear models. Critics including Marvin Minsky pointed out several of the flaws of the linear model family, such as its inability to learn the\n",
    "XOR function, which led to a backlash against the entire neural network approach. \n",
    "\n",
    "Efficient applications of the chain rule based on dynamic programming began to appear in the 1960s and 1970s, mostly for control applications.\n",
    "\n",
    "Following the success of back-propagation, neural network research gained popularity and reached a peak in the early 1990s. Afterwards, other machine learning techniques became more popular until the modern __deep learning__ renaissance that began in 2006.\n",
    "\n",
    "Most of the improvement in neural network performance from 1986 to 2015 can be attributed to two factors. First, __larger datasets__ have reduced the degree to which statistical generalization is a challenge for neural networks. Second, neural networks have become much larger, due to __more powerful computers__, and better software infrastructure. However, a small number of algorithmic changes have improved the performance of neural networks noticeably.\n",
    "\n",
    "One of these algorithmic changes was the __replacement of mean squared error with the cross-entropy__ family of loss functions. The use of cross-entropy losses greatly improved the performance of models with __sigmoid and softmax outputs__, which had previously suffered from saturation and slow learning when using the mean squared error loss.\n",
    "\n",
    "The other major algorithmic change that has greatly improved the performance of feedforward networks was the __replacement of sigmoid hidden units with piecewise linear hidden units__, such as rectified linear units.\n",
    "\n",
    "Jarrett et al. (2009) observed that __using a rectifying nonlinearity is the single most important factor__ in improving the performance of a recognition system among\n",
    "several different factors of neural network architecture design.\n",
    "\n",
    "Glorot et al. (2011a) showed that learning is far easier in deep rectified linear networks than in deep networks that have curvature or two-sided saturation in their activation functions.\n",
    "\n",
    "When the modern resurgence of deep learning began in 2006, feedforward networks continued to have a bad reputation. From about 2006-2012, it was widely believed that feedforward networks would not perform well unless they were assisted by other models. Today, it is now known that with the right resources and engineering practices, feedforward networks perform very well.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
