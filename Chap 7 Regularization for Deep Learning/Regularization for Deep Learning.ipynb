{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chap 7 Regularization for Deep Learning\n",
    "We defined regularization as __any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error.__\n",
    "\n",
    "In the context of deep learning, most regularization strategies are based on regularizing estimators. Regularization of an estimator works by trading increased bias for reduced variance.\n",
    "\n",
    "Deep learning algorithms are typically applied to extremely complicated domains such as images, audio sequences and text, for which the true generation process essentially involves simulating the entire universe. To some extent, we are always trying to fit a square peg (the data generating process) into a round hole (our model family).\n",
    "\n",
    "What this means is that __controlling the complexity of the model__ is NOT a simple matter of finding the __model of the right size__, with the right number of parameters. Instead, we might find and -- indeed in practical deep learning scenarios, we almost always do find -- that the __best fitting model__ (in the sense of minimizing generalization error) is a __large model that has been regularized appropriately.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Parameter Norm Penalties\n",
    "Many regularization approaches are based on limiting the capacity of models by adding a parameter norm penalty $\\Omega(\\theta)$ to the objective function $J$. We denote the regularized objective function by $\\tilde J$\n",
    "\n",
    "$$\\tilde J(\\theta; X,y) = J(\\theta; X,y) + \\alpha \\Omega(\\theta)$$\n",
    "\n",
    "For neural networks, we typically choose to use a parameter norm penalty that penalizes only the weights of the affine transformation at each layer and leaves the biases unregularized. The biases typically require less data to fit accurately than the weights.\n",
    "\n",
    "Each weight specifies how two variables interact. Fitting the weight well requires observing both variables in a variety of conditions. Each __bias controls only a single variable.__ This means that __we do NOT induce too much variance by leaving the biases unregularized.__ Also, __regularizing the bias parameters can introduce a significant amount of underfitting.__\n",
    "\n",
    "In the context of neural networks, it is sometimes desirable to __use a separate penalty with a different α coefficient for each layer of the network.__ Because it can be expensive to search for the correct value of multiple hyperparameters, it is still reasonable to use the same weight decay at all layers just to __reduce the size of search space.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.1 $L^2$ Parameter Regularization\n",
    "L2 parameter norm penalty commonly known as __weight decay__ This regularization strategy __drives the weights closer to the origin__ by adding a regularization term $\\Omega(\\theta) = \\frac{1}{2} ||w||^2$ to the objective function.\n",
    "\n",
    "We can gain some insight into the behavior of weight decay regularization by studying the gradient of the regularized objective function. To simplify the presentation, we assume no bias parameter, so $\\theta$ is just w. \n",
    "\n",
    "__Objective function:__ \n",
    "$$\\tilde J(w;X,y) = \\frac{\\alpha}{2} w^T w + J(w;X,y)$$\n",
    "\n",
    "__Gradient:__ \n",
    "$$\\nabla_w \\tilde J(w;X,y) = \\alpha w + \\nabla_w J(w;X,y)$$\n",
    "\n",
    "__Weight update:__ \n",
    "$$w \\leftarrow w - \\epsilon  (\\alpha w + \\nabla_w J(w; X, y))$$\n",
    "\n",
    "$$w \\leftarrow (1- \\epsilon \\alpha)w - \\epsilon \\nabla_w J(w; X, y)$$\n",
    "\n",
    "We can see that the addition of the weight decay term has modified the learning rule to multiplicatively shrink the weight vector by a __constant factor__ on each step, just before performing the usual gradient update. \n",
    "\n",
    "We will further simplify the analysis by making a __quadratic approximation__ to the objective function in the neighborhood of the value of the weights that obtains minimal unregularized training cost, $w^∗ = \\arg \\min_w J(w)$ \n",
    "\n",
    "> 為了簡化模型，我們將目標函數在最低點做二階的泰勒展開式作為近似。\n",
    "\n",
    "Taylor series expalsion around $w^*$\n",
    "\\begin{align}\n",
    "J(\\theta) & \\approx J(w^*) + (w-w^*)^T g + \\frac{1}{2} (w-w^*)^T H (w-w^*) \\\\\n",
    "&( \\because w^* = arg \\min_w J(w) \\therefore g = 0) \\\\\n",
    "\\hat J(\\theta) & = J(w^{*}) + \\frac{1}{2}(w-w^*)^T H(w-w^*) \\\\\n",
    "\\end{align}\n",
    "\n",
    "> $\\hat J(\\theta)為J(\\theta)在w^*附近之近似$\n",
    "\n",
    "where H is the Hessian matrix of J with respect to w evaluated at $w^*$. Because $w^∗$ is the location of a minimum of J, we can conclude that __H is positive semidefinite.__\n",
    "\n",
    "The minimum of J occurs where its gradient\n",
    "$$\\nabla_w J(w) \\approx \\nabla_w \\hat J(w) = H(w-w^*)$$\n",
    "is equal to zero. \n",
    "\n",
    "We can now solve for the regularized version of J. We use the variable $\\tilde w = arg \\min_w \\tilde J(w)$ to represent the location of the minimum of $\\tilde J$. \n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_w \\tilde J(w) &= \\alpha w + H(w-w^*) \\\\\n",
    "\\Rightarrow &\\alpha \\tilde w + H(\\tilde w - w^*) = 0 \\\\\n",
    "\\Rightarrow & (H+\\alpha I)\\tilde w = H w^* \\\\\n",
    "\\Rightarrow & \\tilde w = (H+\\alpha I)^{-1} H w^* \\\\ \n",
    "\\end{align}\n",
    "\n",
    "As $\\alpha$ approaches 0, the regularized solution $\\tilde w$ approaches $w^*$\n",
    "\n",
    "Because H is __real and symmetric__, we can decompose it into a diagonal matrix D and an orthonormal basis of eigenvectors, Q, such that $H = QDQ^T, QQ^T=Q^TQ=I$\n",
    "\n",
    "\\begin{align}\n",
    "\\tilde w &= (QDQ^T + \\alpha I)^{-1} QDQ^T w^* \\\\ \n",
    "&= (QDQ^T + Q \\alpha Q^T)^{-1} QDQ^T w^* \\\\\n",
    "&= \\left[ Q(D+\\alpha I)Q^T \\right]^{-1} QDQ^T w^* \\\\\n",
    "&= Q(D+\\alpha I)^{-1} DQ^T w^* \\\\\n",
    "\\end{align}\n",
    "\n",
    "We see that the effect of weight decay is to rescale $w^*$ along the axes defined by the eigenvectors of H.\n",
    "\n",
    "Specifically, the component of w∗ that is aligned with the i-th eigenvector of H is rescaled by a factor of $\\frac{\\lambda_i}{\\lambda_i + \\alpha}$\n",
    "\n",
    "Along the directions where the eigenvalues of H are relatively large, the effect of regularization is relatively small. On the other hand, the effect of regularization will shrink eigenvalues to zero.\n",
    "\n",
    "\\begin{align}\n",
    "\\lambda_i >> \\alpha: & \\\\\n",
    "& \\lambda_i' = \\lambda_i \\cdot \\frac{\\lambda_i}{\\lambda_i + \\alpha} \\sim \\lambda_i \\\\\n",
    "\\lambda_i << \\alpha: & \\\\\n",
    "& \\lambda_i' = \\lambda_i \\cdot  \\frac{\\lambda_i}{\\lambda_i + \\alpha} \\sim 0 \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
