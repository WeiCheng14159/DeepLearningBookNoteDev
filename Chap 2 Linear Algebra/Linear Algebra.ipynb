{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chap 2 Linear Algebra\n",
    "\n",
    "參考資料  \n",
    "    [The Matrix Cook Book](files/ref/MatrixCookBook.pdf)\n",
    "### 2.1 純量，向量，矩陣，張量\n",
    "- 純量 略  \n",
    "- 向量 略  \n",
    "- 矩陣 略  \n",
    "- 張量 超過二維的陣列\n",
    "- 矩陣轉置 略\n",
    "- 傳播(Broadcast) \n",
    "在深度學習領域中，有時後會使用非主流的表示方式。  \n",
    "例如：直接用矩陣與向量相加 $C=A+ \\vec{b}$\n",
    "意義為將向量 $\\vec{b}$ 加到 $C$ 中的每一列，這種簡記方式稱為 boradcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 矩陣與向量相乘\n",
    "- 矩陣相乘規則\n",
    "    - 分配律 $A(B+C) = AB+AC$\n",
    "    - 結合律 $A(BC)=(AB)C$\n",
    "    - 沒有交換律 (除非A, B都是向量) \n",
    "        - $AB \\neq BA$\n",
    "        - If $x$ is a vector, $y$ is a vector \n",
    "        - $x \\cdot y = x^{T} y = y^{T} x =$ 純量 \n",
    "\n",
    "- 線性方程組\n",
    "    - $A \\in R^{m \\times n}, \\vec{b} \\in R^{m}, \\vec{x} \\in R^{n \\times 1}$，則一個線性方程組可以寫成$A \\vec{x}=\\vec{b}$  \n",
    "又可以改寫成$A \\vec{x} = [\\vec{A_1} \\vec{A_2} \\dots \\vec{A_n}] \\vec{x} = x_1 \\vec{A_1} +x_2 \\vec{A_2} + \\dots + x_n \\vec{A_n} = \\vec{b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 單位矩陣與逆矩陣\n",
    "- 單位矩陣\n",
    "\n",
    "- 逆矩陣"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 線性相依與展延空間"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__線性方程組的解__  \n",
    "書中直接從Ａ的行數和列數來分析方程組的解，方法略為繁複。這裡假設方程組已經利用列運算化為__列簡梯矩陣(Reduced Row Echelon Form)__後再用矩陣的__秩(Rank)__進行分析。  \n",
    "今有$A \\in R^{m \\times n}, \\vec{b} \\in R^{m}, \\vec{x} \\in R^{n \\times 1}$，$A \\vec{x}=\\vec{b}$ \n",
    "用列運算化簡可以得到列簡梯矩陣 $[A|\\vec{b}]$\n",
    "1. $A \\vec{x}=\\vec{b}$ __無解的條件為__:$rank(A)=r, rank(A|b)=r+1$\n",
    "2. $A \\vec{x}=\\vec{b}$ __恰有一解的條件為__: $rank(A)=rank(A|b)=n$ (A的行數)\n",
    "3. $A \\vec{x}=\\vec{b}$ __無限多組解的條件為__: $rank(A)=rank(A|b) < n$\n",
    "4. $A \\vec{x}=\\vec{b}$ __至少一組解的條件為__: $rank(A)=rank(A|b) \\leq n$ (綜合2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Norms\n",
    "- norm 被用來記算向量的大小，$L^{p}$ 被記為 $$||x||_p=(\\sum_{i} ||{x_i}||^p)^{\\frac{1}{p}}$$  \n",
    "- Norm 是符合以下條件的一種函數:\n",
    "    - $f(x)=0 => x=0$\n",
    "    - $f(x+y) \\leq f(x) + f(y)$ (三角法則)\n",
    "    - $\\forall \\alpha \\in R, f(\\alpha)=\\mid \\alpha \\mid f(x)$  \n",
    "\n",
    "- 常用的norm:  \n",
    "    - $L^2$ norm  \n",
    "        - $L^2$又稱為Euclidean norm, 但是squared $L^2$ norm = $x^T \\cdot x$ 更常用，是$L^2$的平方   \n",
    "\n",
    "        - 因為$L^2$的微分較 squared $L^2$繁複，舉例如下 $$\\vec{x} = [x_1, x_2 \\dots x_n], \\; \\mathbf{P} = x^T x=x_1^2 + x_2^2 \\dots x_n^2, \\; \\mathbf{Q} = \\sqrt{ x^T x } = \\sqrt{x_1^2 + x_2^2 \\dots x_n^2}$$則$$\\frac{\\partial{P}}{\\partial{x_1}} = 2x_1, \\; \\frac{\\partial{Q}}{\\partial{x_1}} = \\frac{x_1}{\\sqrt{x^T x}}$$\n",
    "\n",
    "        - $L^2$的偏微分跟 $\\vec{x}$ 有關，反之squared $L^2$沒有，故後者較簡單較好用\n",
    "\n",
    "    - $L^1$ norm  \n",
    "        - __$L^2$ norm在原點附近對變化不敏感，故用$L^1$ norm較合適__\n",
    "\n",
    "    - max norm  \n",
    "        - 向量的大小使用取絕對值後的最大元素代表\n",
    "\n",
    "    - Frobenius norm \n",
    "        - $||A||_{F}=\\sqrt{ \\sum_{i=1}^m \\sum_{j=1}^n{ |a_{ij}|^2 }} = \\sqrt{Tr(A^TA)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 特殊的矩陣與向量\n",
    "- 對角矩陣(Diagonal Matrix)  \n",
    "    - 對角矩陣之逆矩陣仍然是對角矩陣，且對角線上的元素是原矩陣的倒數\n",
    "    - 對角矩陣之逆矩陣的存在條件為：對角線上所有元素均不為零\n",
    "\n",
    "- 對稱矩陣(Symmetric Matrix)  \n",
    "    - 略  \n",
    "\n",
    "- 歸一正交矩陣(Orthogonal Matrix)  \n",
    "    - 列/行向量都是歸一__正交向量__的方陣\n",
    "    - $A^T A=AA^T=I$\n",
    "    - $A^{-1}=A^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 特徵值分解\n",
    "- 厄米特矩陣之特徵值必為實數，故實對稱矩陣（$\\in$ 厄米特矩陣）之特徵值為實數\n",
    "- 實對稱矩陣可正交對角化  \n",
    "    - 假設A可以特徵值分解則 $A = VDV^{-1}$\n",
    "    - 如果A為實對稱矩陣 $A=A^T$，則A可以正交對角化，Q為正交矩陣  $$A=QDQ^T, \\; Q^{-1}=Q^T$$\n",
    "- 正交對角化之唯一性\n",
    "    - $A$為實對稱矩陣，$A$可以正交對角化\n",
    "    - 若兩個特徵向量對應到同一個特徵值（假設為$\\alpha$），則兩個特徵向量形成之特徵空間$E(\\lambda = \\alpha)$上的任一個向量，都可以是特徵值$\\lambda = \\alpha$之特徵向量。若$A$存在相同之特徵值，則正交對角化不唯一\n",
    "    - $A$之相異特徵值對應之特徵向量正交。若$A$之特徵值皆相異，且$D$對角矩陣之對角線元素有小到大排列，則$A$之正交對角化唯一\n",
    "- 特徵值分解與逆矩陣\n",
    "    - 假設矩陣$A$可以被特徵值分解，則$A$之逆矩陣$A^{-1}$存在的條件為$A$之所有特徵值皆不為零，因為$|A|=\\prod_{i=1}^{i=n}{ \\lambda_i}$\n",
    "- Rayleigh 不等式\n",
    "    - 若$A$為實對稱矩陣，且$f(x) = x^{T} A x$ 且 $||x||=1$\n",
    "    - 則 $\\lambda_{min} \\leq x^{T} A x \\leq \\lambda_{max}$ \n",
    "    - 或可寫成 $\\forall{x}, \\lambda_{min} \\leq \\frac{x^{T} A x}{x^T x} \\leq \\lambda_{max}$ \n",
    "    - 極值條件\n",
    "        - $f(x) = x^{T} A x=\\lambda_{min}$ 之條件為 $x=\\lambda_{min}$ 之特徵向量\n",
    "        - $f(x) = x^{T} A x=\\lambda_{max}$ 之條件為 $x=\\lambda_{max}$ 之特徵向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 奇異值分解\n",
    "- 任何矩陣可以奇異值分解\n",
    "- $A=UDV^T$, $A:m \\times n, \\; U:m \\times m, \\; D:m \\times n, \\; V:n \\times n$\n",
    "- 其中 $U, V$為正交矩陣，$D$之對角元素為奇異值\n",
    "- $U$為$AA^T$之特徵向量，$V$為$A^TA$之特徵向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Pseudoinverse\n",
    "- $A^{+}=VD^{+}U^{T}$\n",
    "- 今有一個任意的線性方程式 $Ax=y, \\; A \\in R^{m \\times n}, x \\in R^{n \\times 1}, \\; y \\in R^{m \\times 1}, \\; rank(A) = r$\n",
    "    - 當$x$有恰一組解，則$x=A^{+}y$為解\n",
    "    - 當$x$有參數解，則$x=A^{+}y$是長度最短的解 $\\vec{x_{mn}}$\n",
    "    - 當$x$無解且$A$行滿秩，則$x=A^{+}y$是唯一的均方近似解 $\\vec{x_{ls}}$\n",
    "    - 當$x$無解且$A$沒有行滿秩，則$x=A^{+}y$是長度最短的均方近似解 $\\vec{x_{ls,mn}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 Trace\n",
    "- $Tr(AB) = Tr(BA) \\; \\forall A, B$為任意矩陣"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11 行列式值\n",
    "- 行列式值是特徵值之積"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.12 Principal Components Analysis \n",
    "- PCA是資料壓縮的方法\n",
    "- 令 $x^{(i)} \\in R^{n}$ 為原向量，$c^{(i)} \\in R^{l}$為壓縮之後的向量。若 $l < n$則可用更少的記憶體存資料\n",
    "    - Encode function $f(x)=c$ \n",
    "    - Decode function $g(c) \\approx x$ 假設 $g(c)=Dc, \\; D \\in R^{n \\times l}$ 且D為正交矩陣 \n",
    "- 壓縮必然有資料損失，PCA的目標為重建的向量與原來的向量之L2 Square Norm最小 \n",
    "\n",
    "\\begin{align}\n",
    "c^{*} & = arg \\min_{c} ||x-g(c)||^{2}_{2} \\\\\n",
    "& \\Leftrightarrow (x-g(c))^T(x-g(c)) \\\\ \n",
    "& \\Leftrightarrow x^T x - x^Tg(c)-g(c)^{T}x+g(c)^Tg(c) \\\\\n",
    "& \\Leftrightarrow x^T x - 2x^Tg(c)+g(c)^Tg(c) \\\\\n",
    "\\end{align}\n",
    "\n",
    "- 去除跟c無關的項目 \n",
    "\n",
    "\\begin{align}\n",
    "c^{*} & = arg \\min_{c} - 2x^Tg(c)+g(c)^Tg(c) \\\\\n",
    "& \\Leftrightarrow c^{*} = arg \\min_{c} - 2x^TDc+c^TD^TDc \\\\\n",
    "& \\Leftrightarrow c^{*} = arg \\min_{c} - 2x^TDc+c^Tc \\\\\n",
    "\\end{align}\n",
    "\n",
    "- 對c找gradient\n",
    "\\begin{align}\n",
    "& \\nabla_{c}(-2x^TDc+c^Tc) = 0 \\\\\n",
    "& \\Leftrightarrow -2D^Tx+2c = 0 \\\\\n",
    "& \\Leftrightarrow c = D^Tx \\\\\n",
    "\\end{align}\n",
    "\n",
    "- 故得Encode Function為 $c=f(x)=D^Tx$, Decode Function為 $r(c)=Dc=DD^Tx$ \n",
    "\n",
    "- 用Frobenius norm來找一個使Encode/Decode後差距最小的正交矩陣D $$D^{*} = arg \\min_{D} \\sqrt{ \\sum_{i,j}{(x_{i,j} - r(x_{i,j}))^2}}, \\; D^TD=I$$\n",
    "- 假設$D=d \\in R^{n \\times 1}$ ，意為Ｄ矩陣會將 $R^{n}$的向量降維到$R^{1}$\n",
    "- 故原式可以寫成 $$ d^{*} = arg \\min_{d} \\sqrt{ \\sum_{i,j}{(x_{i,j} - r(x_{i,j}))^2}}, \\; ||d||_{2}=1$$\n",
    "- 因為$r(c)=dc=dd^Tx$\n",
    "- $$ d^{*} = arg \\min_{d} \\sum_{i}{||x_{i} - x_{i}^Tdd||_{2}^{2}}, \\; ||d||_{2}=1$$\n",
    "- 令$X \\in R^{m \\times n}$ 為所有的輸入向量一列一列做堆疊 $X_{i,:} = {x^{(i)}}^T$\n",
    "- 原式可以寫成 $$ d^{*} = arg \\min_{d} ||X - X^Tdd||_{F}^{2}, \\; ||d||_{2}=1 $$\n",
    "- 根據Frobenius norm的定義 $$ d^{*} = arg \\min_{d} Tr( \\; (X - X^Tdd)^T(X - X^Tdd) \\; )$$\n",
    "- 展開之後得到 $$d^{*} = arg \\min_{d} -Tr(X^TXdd^T)-Tr(dd^TX^TX)+Tr(dd^TX^TXdd^T)$$\n",
    "- 移除和d不相關之項，得到 $$d^{*} = arg \\min_{d} -2Tr(X^TXdd^T)+Tr(dd^TX^TXdd^T)$$\n",
    "- 根據trace的性質$$d^{*} = arg \\min_{d} -2Tr(X^TXdd^T)+Tr(X^TXdd^Tdd^T)$$\n",
    "- 加上 $||d||_{2}=d^Td=1$的條件之後 $$d^{*} = arg \\min_{d} -Tr(X^TXdd^T)$$\n",
    "- 根據trace的性質 $$d^{*} = arg \\min_{d} -Tr(d^TX^TXd)$$\n",
    "- 去掉負號 $$d^{*} = arg \\max_{d} Tr(d^TX^TXd)$$\n",
    "- 假設 $B=X^TX, \\; B^T=(X^TX)^T=X^TX=B$ 為實對稱矩陣\n",
    "- 令 $f(d) = d^TBd, \\; ||d||_2=1$ 根據Rayleigh 不等式 $$\\lambda_{min} \\leq d^{T} B d \\leq \\lambda_{max}$$\n",
    "- 等號成立於d為$X^TX$對應特徵值的特徵向量\n",
    "- 拓展到多維度時，D為$R^{n \\times l}$，D為$X^TX$對應的$\\lambda_{max}$的$\\ell$個特徵向量"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
