{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chap 4 Numerical Computation\n",
    "\n",
    "__Numerical computation__ typically refers to algorithms that solve mathematical problems by methods that update estimates of the solution via an iterative process, rather than analytically deriving a formula providing a symbolic expression for the correct solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Overflow and Underflow \n",
    "The fundamental difficulty in performing continuous math on a digital computer\n",
    "is that we need to represent infinitely many real numbers with a finite number\n",
    "of bit patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Poor Conditioning\n",
    "__Conditioning__ refers to how rapidly a function changes with respect to small changes in its inputs. \n",
    "\n",
    "Consider the function $f(x) = A^{-1} x$. When $A \\in R^{n \\times n}$ has an eigenvalue decomposition. \n",
    "\n",
    "- __Condition number__ is the ratio of the magnitude of the largest and smallest eignevalue. $$\\max_{i,j} | \\frac{\\lambda_i}{\\lambda_i}|$$\n",
    "- When number is __large__, matrix inversion is particularly __sensitive__ to change in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Gradient-Based Optimization\n",
    "#### Gradient descent\n",
    "\n",
    "The function we want to minimize or maximize is called the __objective function__ or __criterion__. \n",
    "\n",
    "We can reduce f (x) by moving x in small steps with opposite sign of the derivative. This technique is called __gradient descent__. \n",
    "\n",
    "The __gradient__ generalizes the notion of derivative to the case where the derivative is with respect to a vector: the gradient of f is the vector containing all of the partial derivatives, denoted $\\nabla_x f (x)$.\n",
    "\n",
    "The __directional derivative__ in direction $\\mu$ (a unit vector) is the slope of the\n",
    "function f in direction $\\mu$. In other words, the directional derivative is the derivative of the function $f (x + \\alpha \\mu)$ with respect to $\\alpha$, evaluated at $\\alpha = 0$. Using the chain rule, we can see that $\\frac{\\partial}{\\partial \\alpha} f(x+\\alpha \\mu)$ evaluates to $\\mu^T \\nabla_x f(x)$ when $\\alpha = 0$\n",
    "\n",
    "To minimize f , we would like to __find the direction in which f decreases the fastest__. We can do this using the directional derivative: \n",
    "\n",
    "$$\\min_{\\mu, \\mu^T \\mu = 1} \\mu^T \\nabla_x f(x)$$ \n",
    "$$= \\min_{\\mu, \\mu^T \\mu = 1} ||\\mu||_2 || \\nabla_x f(x)||_2 \\cos(\\theta)$$ \n",
    "\n",
    "After ignoring factors that don't depend on $\\mu$, this simplifies to $\\min_{\\mu} \\cos(\\theta)$. This is minimized when $\\mu$ points in the opposite direction as the gradient.\n",
    "\n",
    "We can decrease f by moving in the __direction of the negative gradient__. This is known as the method of __steepest descent__ or __gradient descent__. Where $\\epsilon$ is the learning rate. \n",
    "\n",
    "$$x^{'} = x - \\epsilon \\nabla_x f(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 Beyond the Gradient: Jacobian and Hessian Matrices\n",
    "#### Jacobian\n",
    "Sometimes we need to find all of the partial derivatives of a function whose input and output are __both__ vectors.\n",
    "\n",
    "The matrix containing all such partial derivatives is known as a __Jacobian matrix__. Specifically, if we have a function $f:R^m \\rightarrow R^n$, then the __Jacobian matrix__ $J \\in R^{n \\times m}$ of f is defined such that $$J_{i,j} = \\frac{\\partial}{\\partial x_j} f(x)_{i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hessian\n",
    "Many second derivatives can be collected together into a matrix called the __Hessian matrix__. $$H(f)(x)_{i,j} = \\frac{\\partial^2}{\\partial x_i \\partial x_j} f(x)$$\n",
    "\n",
    "Anywhere that the second partial derivatives are __continuous__, the differential operators are __commutative__. This implies that $H_{i,j} = H_{j,i}$ , so the __Hessian matrix is symmetric__ at such points.\n",
    "\n",
    "$$ \\frac{\\partial^2}{\\partial x_i \\partial x_j} f(x) = \\frac{\\partial^2}{\\partial x_j \\partial x_i} f(x) $$\n",
    "\n",
    "Hessian matrix is real and symmetric, we can decompose it into a set of real eigenvalues and an orthogonal basis of eigenvectors. \n",
    "\n",
    "The second derivative in a specific direction represented by a unit vector d is given by \n",
    "\n",
    "$$d^THd$$ \n",
    "\n",
    "When d is an __eigenvector__ of H, the __second derivative__ in that direction is given by the __corresponding eigenvalue__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hessian and Gradient descent \n",
    "We can make a second-order __Taylor series approximation__ to the function f ( x ) around the current point $x^{(0)}$ :\n",
    "\n",
    "$$f(x) \\approx f(x^{(0)}) + (x-x^{(0)})^T g + \\frac{1}{2}(x-x^{(0)})^TH(x-x^{(0)})$$\n",
    "\n",
    "The new point found by gradient descent will be $$ x = x^{(0)} - \\epsilon g$$\n",
    "\n",
    "Plug in to our approximation, \n",
    "\n",
    "$$f(x^{(0)} - \\epsilon g) \\approx f(x^{(0)}) - \\epsilon g^Tg + \\frac{1}{2} \\epsilon^2 g^THg$$\n",
    "\n",
    "The goal of gradient descent is to minimize the cost function f(x). Observe this equation, $g^Tg = ||g||_2^2$ is always positive; however, $g^THg$ might be any positive/zero/negative if H is positive/negative definite. \n",
    "\n",
    "- Case 1: $g^THg$ is __positive__: \n",
    "    - The gradient descent step can actually move uphill and fail to find the minimum of f.  \n",
    "    - Let \n",
    "    \\begin{align} \n",
    "        \\epsilon^{*} & = arg \\min_\\epsilon (- \\epsilon g^Tg + \\frac{1}{2} \\epsilon^2 g^THg ) \\\\\n",
    "        & \\Leftrightarrow \\frac{d}{d \\epsilon} - \\epsilon g^Tg + \\frac{1}{2} \\epsilon^2 g^THg = 0 \\\\\n",
    "        & \\Leftrightarrow - g^Tg + \\epsilon g^THg = 0 \\\\ \n",
    "    \\end{align}\n",
    "    - Solving for the optimal step size $\\epsilon^{*}$ that decreases the Taylor series approximation of the function the most yields $$\\epsilon^{*} = \\frac{g^Tg}{g^THg}$$\n",
    "    \n",
    "    - In the worst case, when g aligns with the eigenvector of H corresponding to the maximal eigenvalue $\\lambda_{max}$, then this optimal step size $\\epsilon^*$ is given by $\\frac{1}{\\lambda_{max}}$. The larger the maximum eigenvalue of H, the smaller the learning rate should be.\n",
    "    - To the extent that the function we minimize can be approximated well by a quadratic function, __the eigenvalues of the Hessian thus determine the scale of the learning rate.__\n",
    "    \n",
    "- Case 2: $g^THg$ is __zero or negative__: \n",
    "    - The taylor series approximation predicts that increasing $\\epsilon$ forever will decrease f forever, which meets our goal to find the minimum of f. \n",
    "    - However, in practice, the Taylor series is unlikely to remain accurate for large $\\epsilon$ (Huge learning rate will make gradient descent unstable), so one must resort to more heuristic choices of $\\epsilon$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariant second derivative test \n",
    "When $f^{'}(x) = 0$ and $f^{''} (x) > 0$, we can conclude that x is a __local minimum__. Similarly, when $f^{'}(x) = 0$ and $f^{''} (x) < 0$, we can conclude that x is a __local maximum__. This is known as the __second derivative test__. Unfortunately, when $f^{''} (x) = 0$, the test is __inconclusive__. In this case x may be a __saddle point__, or a part of a __flat region__.\n",
    "\n",
    "Using the eigendecomposition of the Hessian matrix, we can generalize\n",
    "the __second derivative test__ to multiple dimensions. \n",
    "\n",
    "At a __critical point__, where $\\nabla_x f (x) = 0$, we can examine the eigenvalues of the Hessian to determine whether the critical point is a local maximum, local minimum, or saddle point. \n",
    "\n",
    "When the Hessian is __positive definite__ (all its eigenvalues are positive), the point is a __local minimum__. Likewise, when the Hessian is __negative definite__ (all its eigenvalues are negative), the point is a __local maximum__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multivariant second derivative test \n",
    "In multiple dimensions, it is actually possible to find positive evidence of __saddle points__ in some cases. When at least one eigenvalue is positive and at least one eigenvalue is negative, we know that x is a local maximum on one cross section of f but a local minimum on another cross section.\n",
    "\n",
    "Finally, the __multidimensional second derivative test__ can be __inconclusive__, just like the univariate version. The test is inconclusive whenever all of the non-zero eigenvalues have the same sign, but at\n",
    "least one eigenvalue is zero (__semi positive/negative definite__). This is because the univariate second derivative test is inconclusive in the cross section corresponding to the zero eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Condition number of Hessian matrix\n",
    "The __condition number__ of the Hessian at this point measures how much the second derivatives differ from each other.\n",
    "\n",
    "When the Hessian has a poor condition number, gradient descent performs poorly. This is because in one direction, the derivative increases rapidly, while in another direction, it increases slowly. \n",
    "\n",
    "Gradient descent is unaware of this change in the derivative so it does not know that it needs to explore preferentially in the direction where the derivative remains negative for longer.\n",
    "\n",
    "It also makes it difficult to choose a good step size. The step size must be __small__ enough to __avoid overshooting the minimum__ and __going uphill__ in directions with strong positive curvature. This usually means that the step size is too small to make significant progress in other directions with less curvature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Saddle point\n",
    "![Saddle Point](ref/saddle_point.png)\n",
    "\n",
    "A saddle point containing both positive and negative curvature.\n",
    "\n",
    "In more than one dimension, it is not necessary to have an eigenvalue of 0 in order to get a saddle point: it is only necessary to have both positive and negative eigenvalues. We can think of a saddle point with both signs of eigenvalues as being a local maximum within one cross section and a local minimum within another cross section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient descent \n",
    "![Gradient Descent](ref/gd.png)\n",
    "\n",
    "Gradient descent fails to exploit the curvature information contained in the Hessian matrix.\n",
    "\n",
    "Here we use gradient descent to minimize a quadratic function f(x) whose Hessian matrix has condition number 5. \n",
    "This means that the direction of most curvature has five times more curvature than the direction of least curvature.\n",
    "\n",
    "The large positive eigenvalue of the Hessian corresponding to the eigenvector pointed in this direction indicates that this directional derivative is rapidly increasing, so an optimization algorithm based on the Hessian could predict that the steepest direction is not actually a promising search direction in this context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Newton’s method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convex optimization\n",
    "Convex optimization algorithms are applicable only to convex functions—functions for which the Hessian is positive semidefinite everywhere.\n",
    "Such functions are well-behaved because they lack saddle points and all of their local minima are necessarily global minima.\n",
    "\n",
    "_However, most problems in deep learning are difficult to express in terms of convex optimization._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Constrained Optimization \n",
    "We may wish to find the maximal or minimal value of f(x) for values of x in some set S. This is known as __constrained optimization.__\n",
    "\n",
    "A more sophisticated approach is to design a different, unconstrained optimization problem whose solution can be converted into a solution to the original, constrained optimization problem.\n",
    "\n",
    "We want a description of S in terms of m functions $g^{(i)}$ and n functions $h^{(j)}$ so that S=$\\{ x | \\forall i,g^{(i)}(x)=0 \\; and \\; \\forall j,h^{(j)}(x) \\leq 0 \\}$\n",
    "\n",
    "We introduce new variables $\\lambda_i$ and $\\alpha_j$ for each constraint, these are called the __KKT multipliers__. The generalized Lagrangian is then defined as \n",
    "\n",
    "$$L(x, \\lambda, \\alpha) = f(x) + \\sum_i \\lambda_i g^{(i)}(x) + \\sum_j \\alpha_j h^{(j)} (x)$$\n",
    "\n",
    "We can now solve a constrained minimization problem using unconstrained optimization of the generalized Lagrangian. Observe that, so long as at least one feasible point exists and f(x) is not permitted to have value $\\infty$, then \n",
    "\n",
    "$$\\min_x \\max_{\\lambda} \\max_{\\alpha, \\alpha \\geq 0} L(x, \\lambda, \\alpha) = \\min_{x \\in S} f(x)$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Linear Least Squares\n",
    "Cost function: $$f(x) = \\frac{1}{2} ||Ax-b||_2^2$$\n",
    "Gradient: $$\\nabla_x f(x) = A^T(Ax-b) = A^TAx-A^Tb$$\n",
    "Constraint: $$x^Tx \\leq 1$$\n",
    "Lagrangian: $$L(x, \\lambda) = f(x) + \\lambda(x^Tx-1)$$\n",
    "Constrained cost function: $$\\min_x \\max_{\\lambda \\geq 0} L(x,\\lambda)$$\n",
    "\n",
    "Partial derivative on $\\lambda$:\n",
    "$$\\frac{\\partial}{\\partial \\lambda} L(x,\\lambda) = x^Tx-1$$\n",
    "\n",
    "Partial derivative on x: \n",
    "\\begin{align}\n",
    "    \\frac{\\partial}{\\partial x}L(x, \\lambda) = & \\frac{\\partial}{\\partial x}( A^TAx-A^Tb + \\lambda(x^Tx-1) ) = 0 \\\\\n",
    "    \\Leftrightarrow & A^TAx - A^Tb + 2\\lambda x = 0 \\\\\n",
    "    \\Leftrightarrow & x = (A^TA+2 \\lambda I)^{-1} A^Tb \\\\\n",
    "\\end{align}\n",
    "\n",
    "Observe: \n",
    "\n",
    "Case 1: Constraint violated $x^Tx > 1$\n",
    "$$\\min_x \\max_{\\lambda \\geq 0} L(x,\\lambda) = \\infty$$\n",
    "Case 2: Constraint satisfied $x^Tx \\leq 1$\n",
    "$$\\min_x \\max_{\\lambda \\geq 0} L(x,\\lambda) = f(x)$$\n",
    "\n",
    "So, $$\\min_x \\max_{\\lambda \\geq 0} L(x,\\lambda)$$ and $$\\min_{x, x^Tx \\leq 1}  L(x)$$ have the same set of optimal solution. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
